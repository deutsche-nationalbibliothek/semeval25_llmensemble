general:
  # Settings for Text Embedding model
  embedding_model: BAAI/bge-m3
  collection_name: semeval_GND_Subjects_plus
  vocab: vocab/GND-Subjects-plus.ttl
  # also vectorize SKOS altLabels
  use_altLabels: true
  TEI_port: '8090'
  batch_size: 32 # for embedding model processing of vocab
  # Settings for vLLM
  read_directory: datasets/
  write_directory: results/
  dataset_file: 
    datasets/dev_sample.csv
  split: dev
  sample_size: 1000
  text_types: Article-Book-Conference-Report-Thesis
  subjects: all-subjects
  languages: de-en
  debug: false

vllm:
  engineargs:
    task: generate
    gpu_memory_utilization: 0.95
    tensor_parallel_size: 2
    dtype: float16
    download_dir: /home/huggingface-cache/hub
    enforce_eager: True
    max_model_len: 15000
  global_samplingparams:
    presence_penalty: 0
    frequency_penalty: 0
    repetition_penalty: 1
    top_p: 1

language_models:
- hf_model_name: meta-llama/Llama-3.2-3B-Instruct
  short_name: llama-32-3B
- hf_model_name: mistralai/Mistral-7B-v0.1
  short_name: mistral-7B
- hf_model_name: mistralai/Mistral-7B-Instruct-v0.3
  short_name: mistral-7B-0p3
- hf_model_name: teknium/OpenHermes-2.5-Mistral-7B
  short_name: openhermes-2p5-7B
- hf_model_name: openGPT-X/Teuken-7B-instruct-research-v0.4
  short_name: teuken-7B-0p4
- hf_model_name: meta-llama/Meta-Llama-3.1-70B-Instruct
  short_name: llama31-70B
- hf_model_name: mistralai/Mixtral-8x7B-Instruct-v0.1
  short_name: mistral-8x7B-0p1
completion:
  prompt_specification_files:
  - file: assets/prompts/german/alltypes_de_abstitle_8_0.txt
    short_name: alltypes-de-abstitle-8-0
  - file: assets/prompts/similarity-labelcount/highlemma_fewlabels.txt
    short_name: highlemma-fewlabels
  - file: assets/prompts/similarity-labelcount/highlemma_manylabels.txt
    short_name: highlemma-manylabels
  - file: assets/prompts/similarity-labelcount/lowlemma_fewlabels.txt
    short_name: lowlemma-fewlabels
  - file: assets/prompts/german/alltypes_de_abstitle_8_2.txt
    short_name: alltypes-de-abstitle-8-2
  - file: assets/prompts/german/alltypes_de_abstitle_8_3.txt
    short_name: alltypes-de-abstitle-8-3
  - file: assets/prompts/german/alltypes_de_abstitle_8_4.txt
    short_name: alltypes-de-abstitle-8-4
  - file: assets/prompts/english/english_0_8.txt
    short_name: english-0-8
  - file: assets/prompts/english/english_2_12.txt
    short_name: english-2-12
  - file: assets/prompts/mixed_language/mixed_0_8.txt
    short_name: mixed-0-8
  completion_filename: completions.csv
  batch_size: 20
  prompt_template_info: assets/templates/model_to_template.json
  prompt_template_dir: assets/templates/
  custom_instruction: assets/instructions/instruction01.txt
  text: both # abstract, title, both
  vllm:
    samplingparams:
      max_new_tokens: 100
      min_new_tokens: 24
      max_total_tokens: 15000
      max_chunks: 1
      temperature: 0

mapping:
  output_file: predictions.csv
  n_processes: 20
  # the output will contain a column "score" which is either the cosine similarity or the hybrid score
  score_param: hybrid_score # choice: cosine_similarity, hybrid_score
  hyperparameters:
    search: hybrid        # "hybrid"
    min_cosine_similarity: 0.7 # theshold for vector search
    alpha: 0.7       # 0 - 1

ranking:
  model: meta-llama/Meta-Llama-3.1-8B-Instruct
  score_param: relevance # choice: relevance, count, score
  temperature: 0
  instruction: assets/instructions/ranking_instruction03.json
  max_suggestions: 50
  min_confidence: 0
  threshold_confidence: 0
  max_confidence: 10
  output_file: ranked_predictions.csv

combining_scores:
  weight: 0.30
  output_file: predictions_with_combined_scores.csv

eval:
  ground_truth_path: datasets/
  ground_truth_file: tib-core-subjects-Article-Book-Conference-Report-Thesis-de-dev.csv
  ordinal_ranking: false











